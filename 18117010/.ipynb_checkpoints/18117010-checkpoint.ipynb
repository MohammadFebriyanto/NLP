{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUGAS NLP\n",
    "\n",
    "## Mohammad Febriyanto\n",
    "## 18117010\n",
    "\n",
    "### Berikut ini merupakan algoritma POST TAGGER menggunakan machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(fname):\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    idx_line = 0\n",
    "    while idx_line < len(content):\n",
    "        sent = []\n",
    "        tag = []\n",
    "        while not content[idx_line].startswith('</kalimat'):\n",
    "            if  not content[idx_line].startswith('<kalimat'):\n",
    "                content_part = content[idx_line].split('\\t')\n",
    "                sent.append(content_part[0])\n",
    "                tag.append(content_part[1])\n",
    "            idx_line = idx_line + 1\n",
    "        sentences.append(sent)\n",
    "        tags.append(tag)\n",
    "        idx_line = idx_line+2        \n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(sentence, index):\n",
    "    #sentence: [w1, w2, ....], index: the index of the word\n",
    "    return {\n",
    "        'word' : sentence[index],\n",
    "        'is_first': index==0,\n",
    "        'is_last' : index == len(sentence)-1,\n",
    "        'is_capitalized' : sentence[index][0].upper == sentence[index][0],\n",
    "        'is_all_caps' : sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower' : sentence[index].lower() == sentence[index],\n",
    "        'prefix-1' : sentence[index][0],\n",
    "        'prefix-2' : sentence[index][:2],\n",
    "        'prefix-3' : sentence[index][:3],\n",
    "        'prefix_4' : sentence[index][:4],\n",
    "        'suffix-1' : sentence[index][-1],\n",
    "        'suffix-2' : sentence[index][-2:],\n",
    "        'suffix-3' : sentence[index][-3:],\n",
    "        'prev_word' : '' if index == 0 else sentence[index-1],\n",
    "        'next_word' : '' if index == len(sentence)-1 else sentence[index+1],\n",
    "        '2-prev-token' : '' if index <= 1 else sentence[index - 2],\n",
    "        '2-next-token' : '' if index >= len(sentence) - 2 else sentence[index + 2],\n",
    "        'has_hypen' : '-' in sentence[index],\n",
    "        'is_numeric' : sentence[index].isdigit(),\n",
    "        'capitals_inside' : sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kera', 'untuk', 'amankan', 'pesta olahraga']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences,tags = read_dataset('Indonesian_Manually_Tagged_Corpus_ID.tsv')\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(tagged_sentences):\n",
    "    X = []\n",
    "    \n",
    "    for tagged in tagged_sentences:\n",
    "        X.append([features(tagged, index) for index in range(len(tagged))])\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = dataset(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'Kera',\n",
       "  'is_first': True,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_all_lower': False,\n",
       "  'prefix-1': 'K',\n",
       "  'prefix-2': 'Ke',\n",
       "  'prefix-3': 'Ker',\n",
       "  'prefix_4': 'Kera',\n",
       "  'suffix-1': 'a',\n",
       "  'suffix-2': 'ra',\n",
       "  'suffix-3': 'era',\n",
       "  'prev_word': '',\n",
       "  'next_word': 'untuk',\n",
       "  '2-prev-token': '',\n",
       "  '2-next-token': 'amankan',\n",
       "  'has_hypen': False,\n",
       "  'is_numeric': False,\n",
       "  'capitals_inside': False},\n",
       " {'word': 'untuk',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_all_lower': True,\n",
       "  'prefix-1': 'u',\n",
       "  'prefix-2': 'un',\n",
       "  'prefix-3': 'unt',\n",
       "  'prefix_4': 'untu',\n",
       "  'suffix-1': 'k',\n",
       "  'suffix-2': 'uk',\n",
       "  'suffix-3': 'tuk',\n",
       "  'prev_word': 'Kera',\n",
       "  'next_word': 'amankan',\n",
       "  '2-prev-token': '',\n",
       "  '2-next-token': 'pesta olahraga',\n",
       "  'has_hypen': False,\n",
       "  'is_numeric': False,\n",
       "  'capitals_inside': False},\n",
       " {'word': 'amankan',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_all_lower': True,\n",
       "  'prefix-1': 'a',\n",
       "  'prefix-2': 'am',\n",
       "  'prefix-3': 'ama',\n",
       "  'prefix_4': 'aman',\n",
       "  'suffix-1': 'n',\n",
       "  'suffix-2': 'an',\n",
       "  'suffix-3': 'kan',\n",
       "  'prev_word': 'untuk',\n",
       "  'next_word': 'pesta olahraga',\n",
       "  '2-prev-token': 'Kera',\n",
       "  '2-next-token': '',\n",
       "  'has_hypen': False,\n",
       "  'is_numeric': False,\n",
       "  'capitals_inside': False},\n",
       " {'word': 'pesta olahraga',\n",
       "  'is_first': False,\n",
       "  'is_last': True,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_all_lower': True,\n",
       "  'prefix-1': 'p',\n",
       "  'prefix-2': 'pe',\n",
       "  'prefix-3': 'pes',\n",
       "  'prefix_4': 'pest',\n",
       "  'suffix-1': 'a',\n",
       "  'suffix-2': 'ga',\n",
       "  'suffix-3': 'aga',\n",
       "  'prev_word': 'amankan',\n",
       "  'next_word': '',\n",
       "  '2-prev-token': 'untuk',\n",
       "  '2-next-token': '',\n",
       "  'has_hypen': False,\n",
       "  'is_numeric': False,\n",
       "  'capitals_inside': False}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.tag.util import untag\n",
    "\n",
    "#split dataset for train and test\n",
    "cutoff = int(.75 * len(sentences))\n",
    "X_train = sentences[:cutoff]\n",
    "y_train = tags[:cutoff]\n",
    "X_test = sentences[cutoff:]\n",
    "y_test = tags[cutoff:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/febriyanto/.local/lib/python3.6/site-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=True, all_possible_transitions=True,\n",
       "    averaging=None, c=None, c1=0.1, c2=0.1, calibration_candidates=None,\n",
       "    calibration_eta=None, calibration_max_trials=None, calibration_rate=None,\n",
       "    calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "    gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=300,\n",
       "    max_linesearch=None, min_freq=None, model_filename=None, num_memories=None,\n",
       "    pa_type=None, period=None, trainer_cls=None, variance=None, verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "model = CRF(algorithm='lbfgs', \n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    all_possible_states=True,\n",
    "    max_iterations=300,\n",
    "    all_possible_transitions=True)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9662413103513952\n"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(metrics.flat_accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
